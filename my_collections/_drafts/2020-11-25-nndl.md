## Neural Networks & Deep Learning

### Activation
 - We call a neuron active when it's "lit up!"
 - Activations of input layer depends on how input is encoded & fed into the network
 - Activations of output layer are effectively, the predictions/choices the network is making
 - Activations of inner layers 

 - Activations of one layer decides activations in the next layer. How?
    - Find Weighted sum of all activations of previous layer 
    - 'Squish' the sum into a range between 0 & 1. 
      




Neural Network as a Function
- Input: 
- Parameters:
- Output:
Cost Function as a ...well, Function
- Input:
- Parameters:
- Output:

### Gradient Descent
What's Gradient Descent's purpose? 
To find minima of cost function. 

Simple calculus of finding points where ∂C/∂w = 0 won't work?  
No, definitely not for function with potentially millions of parameters.

How does Gradient Descent work ? 
Start at a random point & figure out next step that will lower the functions value. 

Ok, but how?
```
1. Find slope at a random initial point. 
2. Move to the left if slope is +ve. Move to the right if slope is -ve. 
3. Keep moving till we reach a point of 0 slope. That's our minima! 
```
Pro Tip: If we can move with step sizes proportional to the slope, we won't re-bounce off the minima. 

Sounds simple, but what's the catch? 
The minima we're gonna reach is Local. It depends on the random initial point you started with. 


- All you need to remember is there exists a way to know what the downhill direction is & how steep it is. 
    - tangent at a point
    <img>
    - when there are 2 variables
    <img>
    - scale it to n variables
    <img>
What's BackProp?


### Links
 - 3Blue1Brown series on Neural Networks
 - Michael Nielson's NNDL e-book